{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbhiramNallamothu/excercies_5731/blob/main/Abhiram_Nallamothu_Exercise_3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "0e4abcf7-ad8e-4c61-f233-0a80f37000f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nOne such interesting text classification task is movie reviews classification. Natural language processing (NLP) is the field that deals with automatically\\nclassifying movie reviews into specified sentiment categories, which are often positive or negative.\\nThe goal is to develop machine learning models that can accurately\\npredict the sentiment expressed in a given review. This exercise is a\\nparticular application of sentiment analysis to movie reviews, with a variety of real-world uses, including:\\nImproving Recommendation Systems: By suggesting movies with favorable\\nreviews to viewers who are likely to enjoy them, sentiment analysis of movie reviews improves recommendation systems.\\n\\nIn this task, the features are listed below:\\n\\n1. Word Frequency (Bag of Words):\\n    - Determine how often a particular word appears in a review.\\n    - Helps identify prevalent feelings by identifying recurring themes or subjects in reviews.\\n2. Term Frequency-Inverse Document Frequency(TF-IDF):\\n    - Emphasize important, uncommon terms in a review.\\n    - Highlights unique phrases with a particular sentiment to improve classification accuracy.\\n3. Word Groups (N-grams):\\n    - To gain further context, look at neighboring word pairs.\\n    - Provides a contextual understanding through word pair analysis, identifying complex emotions.\\n4. Word Embeddings:\\n    - Understand word meanings in connection to one another.\\n    - Promotes a more sophisticated comprehension of language by grasping the semantic links between words.\\n5. Negative Sentiment Recognition:\\n    - Look for terms that reverse feelings, such as \"not good.\"\\n    - Recognizes subtleties in language that point to a reversal of sentiment and is essential for precise classification.\\n6. Punctuation Signals:\\n    - Look for indications of sentiment in capitalization or punctuation.\\n    - It will identify emotional intensity through capitalization or punctuation.\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "One such interesting text classification task is movie reviews classification. Natural language processing (NLP) is the field that deals with automatically\n",
        "classifying movie reviews into specified sentiment categories, which are often positive or negative.\n",
        "The goal is to develop machine learning models that can accurately\n",
        "predict the sentiment expressed in a given review. This exercise is a\n",
        "particular application of sentiment analysis to movie reviews, with a variety of real-world uses, including:\n",
        "Improving Recommendation Systems: By suggesting movies with favorable\n",
        "reviews to viewers who are likely to enjoy them, sentiment analysis of movie reviews improves recommendation systems.\n",
        "\n",
        "In this task, the features are listed below:\n",
        "\n",
        "1. Word Frequency (Bag of Words):\n",
        "    - Determine how often a particular word appears in a review.\n",
        "    - Helps identify prevalent feelings by identifying recurring themes or subjects in reviews.\n",
        "2. Term Frequency-Inverse Document Frequency(TF-IDF):\n",
        "    - Emphasize important, uncommon terms in a review.\n",
        "    - Highlights unique phrases with a particular sentiment to improve classification accuracy.\n",
        "3. Word Groups (N-grams):\n",
        "    - To gain further context, look at neighboring word pairs.\n",
        "    - Provides a contextual understanding through word pair analysis, identifying complex emotions.\n",
        "4. Word Embeddings:\n",
        "    - Understand word meanings in connection to one another.\n",
        "    - Promotes a more sophisticated comprehension of language by grasping the semantic links between words.\n",
        "5. Negative Sentiment Recognition:\n",
        "    - Look for terms that reverse feelings, such as \"not good.\"\n",
        "    - Recognizes subtleties in language that point to a reversal of sentiment and is essential for precise classification.\n",
        "6. Punctuation Signals:\n",
        "    - Look for indications of sentiment in capitalization or punctuation.\n",
        "    - It will identify emotional intensity through capitalization or punctuation.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4e2b91-1cc3-443f-eee2-685c2aa6ff51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.10.1)\n",
            "/usr/lib/python3.10/runpy.py:126: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2024.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Tokens:  ['i', 'loved', 'the', 'movie', '!', 'the', 'plot', 'was', 'amazing', ',', 'and', 'the', 'acting', 'was', 'superb', '.', 'highly', 'recommended', '!', 'ðŸ‘Œ']Word Frequency:  {'i': 1, 'loved': 1, 'the': 3, 'movie': 1, '!': 2, 'plot': 1, 'was': 2, 'amazing': 1, ',': 1, 'and': 1, 'acting': 1, 'superb': 1, '.': 1, 'highly': 1, 'recommended': 1, 'ðŸ‘Œ': 1}\n",
            "Bigrams: [('i', 'loved'), ('loved', 'the'), ('the', 'movie'), ('movie', '!'), ('!', 'the'), ('the', 'plot'), ('plot', 'was'), ('was', 'amazing'), ('amazing', ','), (',', 'and'), ('and', 'the'), ('the', 'acting'), ('acting', 'was'), ('was', 'superb'), ('superb', '.'), ('.', 'highly'), ('highly', 'recommended'), ('recommended', '!'), ('!', 'ðŸ‘Œ')]\n",
            "Sentiment Score: {'neg': 0.0, 'neu': 0.47, 'pos': 0.53, 'compound': 0.9244}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "!pip install emoji\n",
        "!python -m nltk.downloader punkt\n",
        "!pip install vaderSentiment\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import emoji\n",
        "import re\n",
        "# Sample movie review\n",
        "review = \"I loved the movie! The plot was amazing, and the acting was superb. Highly recommended!ðŸ‘Œ\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(review.lower())\n",
        "print(\"Tokens: \",tokens,end=\"\")\n",
        "\n",
        "# Word Frequency (Bag of Words)\n",
        "word_freq = {}\n",
        "for word in tokens:\n",
        "    word_freq[word] = word_freq.get(word, 0) + 1\n",
        "print(\"Word Frequency: \", word_freq,end=\"\\n\")\n",
        "\n",
        "# Word Groups (N-grams)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(\"Bigrams:\", bigrams,end=\"\\n\")\n",
        "\n",
        "# Sentiment Analysis using SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sentiment_score = sia.polarity_scores(review)\n",
        "print(\"Sentiment Score:\", sentiment_score,end=\"\\n\")\n",
        "\n",
        "# Negative Sentiment Recognition\n",
        "negative_words = [\"not\", \"never\", \"bad\"]\n",
        "for word in negative_words:\n",
        "    if word in tokens:\n",
        "        print(f\"Negative sentiment indicator found: {word}\",end=\"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9791e3a9-610b-4b80-9950-19f079b3a7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked terms based on correlation coefficients:\n",
            "             Correlation Coefficient\n",
            "acting                        0.9244\n",
            "amazing                       0.9244\n",
            "highly                        0.9244\n",
            "loved                         0.9244\n",
            "movie                         0.9244\n",
            "plot                          0.9244\n",
            "recommended                   0.9244\n",
            "superb                        0.9244\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Sample movie review\n",
        "review = \"I loved the movie! The plot was amazing, and the acting was superb. Highly recommended!ðŸ‘Œ\"\n",
        "\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "TF_IDF_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit the vectorizer on the review text data\n",
        "X_fitting_TF_IDF_vectorizer = TF_IDF_vectorizer.fit_transform([review])  # Pass the review as a list containing a single string\n",
        "\n",
        "# Compute the correlation coefficient between each feature and the target variable (sentiment score)\n",
        "correlation_coefficients = {}\n",
        "for term in TF_IDF_vectorizer.get_feature_names_out():\n",
        "    if term in tokens:\n",
        "        correlation_coefficients[term] = sentiment_score['compound']\n",
        "    else:\n",
        "        correlation_coefficients[term] = 0  # If term not present in review, set correlation coefficient to 0\n",
        "\n",
        "# Create a DataFrame to store correlation coefficients\n",
        "df_correlation = pd.DataFrame.from_dict(correlation_coefficients, orient='index', columns=['Correlation Coefficient'])\n",
        "\n",
        "# Rank the terms based on their correlation coefficients\n",
        "ranked_terms = df_correlation.sort_values(by='Correlation Coefficient', ascending=False)\n",
        "\n",
        "# Display the ranked terms\n",
        "print(\"Ranked terms based on correlation coefficients:\")\n",
        "print(ranked_terms)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1GgDWZ483q",
        "outputId": "a711caee-f127-44eb-b3a3-0899c93aefda"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.38.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence-transformers) (0.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "model = SentenceTransformer('bert-base-uncased')\n",
        "\n",
        "# Example text and data for question 2\n",
        "text_data = [\n",
        "    \"I enjoyed the movie, it was great!\",\n",
        "    \"The plot of the movie was confusing, but the acting was good.\",\n",
        "    \"Highly recommended film with an interesting storyline.\",\n",
        "    \"The movie was boring, and the acting was mediocre.\",\n",
        "]\n",
        "\n",
        "query = \"I loved the movie! The plot was amazing, and the acting was superb. Highly recommended!ðŸ‘Œ\"\n",
        "\n",
        "# Encode the query and text data to get BERT embeddings\n",
        "query_embedding = model.encode(query, convert_to_tensor=False)  # Convert to numpy array\n",
        "\n",
        "text_embeddings = [model.encode(sentence, convert_to_tensor=False) for sentence in text_data]\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "similarities = [cosine_similarity([query_embedding], [text_embedding])[0][0] for text_embedding in text_embeddings]\n",
        "\n",
        "# Create a list of tuples containing the index and similarity score\n",
        "similarity_scores = list(enumerate(similarities))\n",
        "\n",
        "# Sort the list based on similarity scores in descending order\n",
        "sorted_similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Display the ranked text data based on similarity\n",
        "for index, score in sorted_similarity_scores:\n",
        "    print(f\"Similarity Score: {score:.4f} - Text: {text_data[index]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikZrwqax49Vz",
        "outputId": "1d25ac81-f60c-47e4-f4f4-1b0cf0ad60c8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Score: 0.8269 - Text: I enjoyed the movie, it was great!\n",
            "Similarity Score: 0.8260 - Text: The plot of the movie was confusing, but the acting was good.\n",
            "Similarity Score: 0.7458 - Text: Highly recommended film with an interesting storyline.\n",
            "Similarity Score: 0.7450 - Text: The movie was boring, and the acting was mediocre.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "\n",
        "Selecting the proper feature selection techniques was difficult for me when I was answering the third question.\n",
        "Since the goal of this procedure is to extract characteristics from data to assess relevance or the emotional context of a given review, each stage is critical to natural language processing (NLP).\n",
        "Overall, the procedures used here are quite pertinent to natural language processing, which makes comprehending and utilizing feature extraction techniques easier.\n",
        "It has been a bit hard for me while doing the process of text mining. I had to spend more time on this learning\n",
        "The resources have shown to be quite beneficial in helping to understand ideas around feature extraction and selection.\n",
        "Application for the Bag of Words, tokenization and word groups model includes sentiment analysis, information retrieval, and text classification.\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}